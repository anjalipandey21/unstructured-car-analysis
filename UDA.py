# -*- coding: utf-8 -*-
"""Assignment_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EziTVppjhasFJbZSLzrBMIK9nnm0BLG4

<h1 style="text-align:center;"><u><b>Unstructured Data Analytics - Assignment 1<b></u></h1>

Team Members:

Sukhada Virkar, Ruizhi Ding, Abhiram Iyengar, Anshul Joshi, Akhil Sourav
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import pandas as pd
from collections import Counter
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import linregress
import re
from collections import Counter
import pandas as pd
import re
from collections import Counter
import numpy as np
import matplotlib.pyplot as plt
from patsy import dmatrices
import statsmodels.api as sm

"""# The below code scrapes all the reviews from the given URL"""

# Base URL for the forum
base_url = 'https://forums.edmunds.com/discussion/2864/general/x/entry-level-luxury-performance-sedans'

# A list to hold scraped data
data = []
post_counter = 0
post_limit = 5000

# Function to scrape data from a single page
def scrape_page(page_url):
    global post_counter # Declare post_counter as global
    response = requests.get(page_url)

    if response.status_code == 200:
        soup = BeautifulSoup(response.content, 'html.parser')

        # Find all post containers for reviews
        posts = soup.find_all('div', class_='Comment')
        print(f"Found {len(posts)} posts on the page.")  # Add this line to print the number of posts found

        # Loop through all posts found on the page
        for post in posts:
            # Extract the username from <a class="Username">

            #print(f"Processing post: {post}")  # Add this line to print the post being processed
            user_tag = post.find('a', class_='Username')
            user_name = user_tag.get_text(strip=True) if user_tag else 'N/A'


            # Extract number of posts from <span class="MItem PostCount">
            #post_count_tag = post.find('span', class_='PostCount')
            #number_of_posts = post_count_tag.get_text(strip=True) if post_count_tag else 'N/A'
            #print(f"Number of Posts: {number_of_posts}")  # Add this line to print the extracted number of posts

            # Extract the date of the post from <time> tag
            date_tag = post.find('time')
            date = date_tag.get_text(strip=True) if date_tag else 'N/A'
            # print(f"Date: {date}")  # Add this line to print the extracted date

            # Extract the review from <div class="Message userContent">
            review_tag = post.find('div', class_='Message userContent')
            review = review_tag.get_text(strip=True) if review_tag else 'N/A'
            # print(f"Review: {review}")  # Add this line to print the extracted review

            # Append the data to the list
            if user_name != 'N/A' and review != 'N/A':  # Add only if data is valid
                data.append([user_name, date, review])
                # print(f"Data added to the list: {data}")
                post_counter += 1
                if post_counter >= post_limit:
                    return  # Stop if the post limit is reached
    else:
        print(f"Failed to retrieve page: {page_url}")

# Function to get the previous page URL
def get_previous_page(soup):
    previous_button = soup.find('a', class_='Previous')
    if previous_button:
        return previous_button['href']
    return None

## Initial page URL
#page_url = base_url

# Initial page URL (last page)
page_url = f'{base_url}/p435'

# Loop through pages and scrape data
while page_url:
    # print(f"Scraping page: {page_url}")
    scrape_page(page_url)
    if post_counter >= post_limit:
        break

    # Get the next page link #previous page rn
    response = requests.get(page_url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.content, 'html.parser')
        #page_url = get_next_page(soup)  # Get next page link if available
        page_url = get_previous_page(soup)  # Get previous page link if available
    else:
        break

    # Pause between requests to avoid overwhelming the server
    time.sleep(5)

# Convert the data into a DataFrame
df = pd.DataFrame(data, columns=['Username', 'Date', 'Review'])

# Save the data to a CSV file
df.to_csv('edmunds_forum_posts.csv', index=False)

print("Scraping completed and data saved to edmunds_forum_posts.csv")

data = pd.read_csv('/content/edmunds_forum_posts.csv')

"""# TASK A"""

# Tokenizing the text using basic regex (splitting by non-alphabetic characters)
all_reviews = ' '.join(data['Review'].astype(str).tolist()).lower()
words = re.findall(r'\b\w+\b', all_reviews)

# Counting the frequency of each word
word_freq = Counter(words)

# Display the most common words and their frequencies
word_freq.most_common(100)

"""**Above displays the 100 most frequent words found in the dataset along with their occurrences.**

During the pre-processing stage, the text was tokenized, and all words were converted to lowercase. As requested, stop words were retained, and neither stemming nor lemmatization was applied.

In essence, Zipf's law predicts that a word's frequency is inversely related to its rank. Therefore, an additional dataframe is created with a column representing these theoretical frequencies to facilitate plotting and testing.

A new column with theoretical frequencies based on Zipf's law has been added.
"""

# Rank the words by frequency
ranked_words = word_freq.most_common()

# Extract the top 100 words and their empirical frequencies
top_100_words = ranked_words[:100]

# Creating a DataFrame for the top 100 words
df2 = pd.DataFrame(top_100_words, columns=['Word', 'Frequency'])

# Adding Rank column based on the index (rank starts at 1, not 0)
df2['Rank'] = df2.index + 1

# Calculate the Zipf's theoretical frequency
f1 = df2['Frequency'].iloc[0]  # Frequency of the most common word
df2['Zipf_Theoretical_Frequency'] = f1 / df2['Rank']

"""**Plotting word frequencies against the theoretical predictions based on Zipf's law**"""

# Load the CSV data (Replace with your path if different)
data = pd.read_csv('edmunds_forum_posts.csv')

# Tokenizing the text using basic regex (splitting by non-alphabetic characters)
all_reviews = ' '.join(data['Review'].astype(str).tolist()).lower()
words = re.findall(r'\b\w+\b', all_reviews)

# Counting the frequency of each word
word_freq = Counter(words)

# Rank the words by frequency
ranked_words = word_freq.most_common()

# Extract the top 100 words and their empirical frequencies
top_100_words = ranked_words[:100]
empirical_freq = [freq for word, freq in top_100_words]
top_100_labels = [word for word, freq in top_100_words]

# Theoretical frequencies based on Zipf's law (1 / rank)
ranks = np.arange(1, 101)
theoretical_freq = [empirical_freq[0] / r for r in ranks]  # scale to first word's frequency

# Create the plot
plt.figure(figsize=(14, 7))

# Plot the actual word frequencies as a bar plot
plt.bar(top_100_labels, empirical_freq, color='blue', label='Actual Frequencies')

# Plot the Zipf's law prediction as a red dashed line with circular markers
plt.plot(top_100_labels, theoretical_freq, 'ro-', label="Zipf's Law Prediction", linestyle='--')

# Set labels and titles
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.title("Top 100 Words: Actual Frequencies vs Zipf's Law Prediction")
plt.xticks(rotation=90)
plt.legend()

# Show the plot
plt.tight_layout()
plt.show()

"""**Testing of Zipf's law econometrically on the top 100 words**

To test this we need to take the log transformation of both the rank and frequency

According to Zipf's law, the relationship between the logarithm of rank and frequency should be approximately linear, with a slope of -1 and hence we perform linear regression for the same
"""

# Calculate log values
df2['Log_Rank'] = np.log(df2['Rank'])
df2['Log_Frequency'] = np.log(df2['Frequency'])

# Perform OLS regression
y, X = dmatrices('Log_Frequency ~ Log_Rank', data=df2, return_type='dataframe')
model = sm.OLS(y, X)
results = model.fit()

# Print the OLS regression summary
print(results.summary())

"""**Plotting to test Zipf's law econometrically**"""

# Plotting the log-log plot to test Zipf's law econometrically
plt.figure(figsize=(10, 6))

# Scatter plot of log(rank) vs log(frequency) for the top 100 words
plt.scatter(df2['Log_Rank'][:100], df2['Log_Frequency'][:100], label='Data', color='blue')

# Fitted line from the OLS regression model
plt.plot(df2['Log_Rank'][:100], results.predict(X)[:100], 'r',
         label=f'Fitted line (slope = {results.params[1]:.2f})')

# Labeling the axes and title
plt.xlabel('Log Rank')
plt.ylabel('Log Frequency')
plt.title("Log-Log Plot of Rank vs Frequency (Zipf's Law Test)")

# Add a legend
plt.legend()

# Improve layout
plt.tight_layout()

# Display the plot
plt.show()

"""# Task B

Replace frequently occurring car models with
brands
"""

import csv
import re
import string
from collections import Counter
from nltk.corpus import stopwords
import nltk

# Ensure you have downloaded the stopwords using nltk
nltk.download('stopwords')


# Initialize English stopwords and add any additional words you'd like to exclude
stop_words = set(stopwords.words('english'))
additional_stopwords = {'car','problem','seat','sedan'}  # Add any other non-brand words
stop_words.update(additional_stopwords)

# Function to clean and tokenize sentences, filtering out non-brand words
def preprocess_tokens(sentence):
    """
    Cleans a given sentence by removing punctuation, stopwords, and tokenizing the remaining words.
    Only keeps brand names from the provided list.
    """
    # Remove punctuation and convert text to lowercase
    sentence = re.sub(f'[{re.escape(string.punctuation)}]', '', sentence.lower())

    # Tokenize and remove stopwords, only keeping car brand names
    return [word for word in sentence.split() if word not in stop_words]



# Step 2:  Pre process sentences from the text
def pre_process_sentences(text):

    sentences_clean = []

    sentences = re.split(r'[.!?]', text)  # Split based on sentence-ending punctuation
    for sentence in sentences:
        cleaned_tokens = preprocess_tokens(sentence)
        if cleaned_tokens:  # Avoid adding empty sentences
          cleaned_sentence = ' '.join(cleaned_tokens)
          sentences_clean.append(cleaned_sentence)

    return ' '.join(sentences_clean)

import csv
import re
import shutil
from tempfile import NamedTemporaryFile

# Filepaths
output_file = '/content/cleaned_replaced_records_forum.csv'  # The file where the modified data will be stored
# input_file = '/content/scraped_data/sample_forum_posts.csv'  # The file containing the original data
input_file = '/content/edmunds_forum_posts.csv'
replacement_file = '/content/car_models_and_brands.csv'  # The file containing original and replacement words

# Create a temporary file to write the changes before moving it to the final location
tempfile = NamedTemporaryFile(mode='w', delete=False, newline='', encoding='utf-8')

def load_replacements(replacement_file):
  """
  Load word replacements from a CSV file into a dictionary.
  The right column contains words to be replaced by the corresponding words in
  the left column.
  """
  replacements = {}
  with open(replacement_file, 'r', encoding='utf-8') as csvfile:
    reader = csv.reader(csvfile, delimiter=',', quotechar='"')
    for row in reader:
      original, replacement = row[1].lower(), row[0].lower() # Ensure lowercase comparison
      replacements[original] = replacement
  return replacements


def replace_model_with_brand(text, replacements):
    """
    Replace car models in the forum post (text) with the corresponding brands using the replacements dictionary.
    """
    words = text.lower().split()  # Split the text into words

    replaced_words = [replacements[word] if word in replacements else word for word in words]

    return ' '.join(replaced_words)

def process_file(input_file, output_file, replacements):
    """
    Read the input file, perform model-to-brand replacements in the forum messages,
    and write the modified content to the output file.
    """
    with open(input_file, 'r', encoding='utf-8') as csvfile, tempfile:
        reader = csv.reader(csvfile)
        writer = csv.writer(tempfile)

        # Write the header to the temp file
        header = next(reader)
        writer.writerow(header)  # Write unchanged header

        # Process each row in the input CSV
        for row in reader:
            user, date, message = row[0], row[1], row[2]

            message_clean = pre_process_sentences(message)

            # Replace models with the corresponding brands in the message using the replacement dictionary
            modified_message = replace_model_with_brand(message_clean, replacements)
            writer.writerow([user, date, modified_message])

    # Replace the original file with the modified file
    shutil.move(tempfile.name, output_file)

if __name__ == "__main__":
    # Load the replacement words from the replacement CSV file
    replacements = load_replacements(replacement_file)

    # Process the input file and apply the replacements
    process_file(input_file, output_file, replacements)

"""Find the top 10 brands from frequency counts."""

# Input and output filenames
input_filename = '/content/cleaned_replaced_records_forum.csv'  # Input file path
word_freq_output = '/content/word_freq.csv'  # Output file for word frequencies
brand_filename = '/content/car_models_and_brands.csv'  # Path to the car brand list

# Step 1: Load car brand names from the CSV file
def load_car_brands(brand_file):
    """
    Loads car brands from a CSV file into a set for quick lookup.
    """
    car_brands = set()

    with open(brand_file, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        next(reader)  # Skip header

        for row in reader:
            if len(row) > 0:
                car_brands.add(row[0].lower())  # Assuming brand name is in the first column

    return car_brands

# Load the car brands list
car_brands = load_car_brands(brand_filename)

# Function to clean and tokenize sentences, filtering out non-brand words
def clean_and_tokenize(sentence):
    """
    Cleans a given sentence by removing punctuation, stopwords, and tokenizing the remaining words.
    Only keeps brand names from the provided list.
    """
    # Remove punctuation and convert text to lowercase
    sentence = re.sub(f'[{re.escape(string.punctuation)}]', '', sentence.lower())

    # Tokenize and remove stopwords, only keeping car brand names
    return [word for word in sentence.split() if word in car_brands and word not in stop_words]


# Step 2: Extract and clean sentences from the text
def extract_sentences(file):
    """
    Extracts text data from the third column of the CSV file, splits it into sentences,
    and cleans each sentence by removing punctuation and stopwords.
    """
    sentences_clean = []

    with open(file, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        next(reader)  # Skip header

        for row in reader:
            if len(row) >= 3:
                text = row[2]  # Assuming the third column contains the text data
                sentences = re.split(r'[.!?]', text)  # Split based on sentence-ending punctuation
                for sentence in sentences:
                    cleaned_tokens = clean_and_tokenize(sentence)
                    if cleaned_tokens:  # Avoid adding empty sentences
                        sentences_clean.append(cleaned_tokens)

    return sentences_clean

# Step 3: Calculate word frequencies
def calculate_word_frequencies(sentences):
    """
    Calculates the frequency of each word in the given list of cleaned sentences.
    """
    word_freq = Counter()

    for sentence in sentences:
        word_freq.update(sentence)

    return word_freq

# Step 4: Write word frequencies to CSV
def write_word_frequencies(word_freq, output_file):
    """
    Writes the word frequencies to the specified CSV file.
    """
    with open(output_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['Brand', 'Frequency'])
        for word, freq in word_freq.most_common(10):  # Get top 10 brands
            writer.writerow([word, freq])

    print(f"Top 10 brand frequencies written to {output_file}")

# Main function to run the process
def main():
    sentences_clean = extract_sentences(input_filename)
    word_freq = calculate_word_frequencies(sentences_clean)
    write_word_frequencies(word_freq, word_freq_output)

if __name__ == "__main__":
    main()

import pandas as pd
word_freq_df = pd.read_csv('/content/word_freq.csv')
word_freq_df

"""# TASK C"""

import csv
import re
import string
from collections import Counter, defaultdict
from nltk.corpus import stopwords
import nltk
import matplotlib.pyplot as plt
import numpy as np

# Ensure you have downloaded the stopwords using nltk
nltk.download('stopwords')

# Initialize English stopwords and add any additional words you'd like to exclude
stop_words = set(stopwords.words('english'))
additional_stopwords = {'car', 'problem', 'seat','sedan'}  # Add any other non-brand words
stop_words.update(additional_stopwords)

# Input filenames
# input_filename = '/content/sample_forum_posts.csv'  # Input file path
input_filename = '/content/cleaned_replaced_records_forum.csv'
window_size = 7  # Define the window size for proximity between brands

# Load car brands
def load_car_brands(brand_file):
    """
    Loads car brands from a CSV file into a set for quick lookup.
    """
    car_brands = set()

    with open(brand_file, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        next(reader)  # Skip header if there is one

        for row in reader:
            if len(row) > 0:
                car_brands.add(row[0].lower())  # Assuming brand name is in the first column

    return car_brands

# Load the car brands from the uploaded CSV file
car_brands = load_car_brands('/content/word_freq.csv')
print(car_brands)

# Function to clean and tokenize sentences, filtering out non-brand words
def clean_and_tokenize(sentence):
    """
    Cleans a given sentence by removing punctuation, stopwords, and tokenizing the remaining words.
    Only keeps brand names from the provided list.
    """
    # Remove punctuation and convert text to lowercase
    sentence = re.sub(f'[{re.escape(string.punctuation)}]', '', sentence.lower())

    # Tokenize and remove stopwords, only keeping car brand names
    return [word for word in sentence.split() if word in car_brands and word not in stop_words]

# Step 2: Extract and clean sentences from the text
def extract_sentences(file):
    """
    Extracts text data from the third column of the CSV file, splits it into sentences,
    and cleans each sentence by removing punctuation and stopwords.
    """
    sentences_clean = []

    with open(file, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        next(reader)  # Skip header

        for row in reader:
            if len(row) >= 3:
                text = row[2]  # Assuming the third column contains the text data
                sentences = re.split(r'[.!?]', text)  # Split based on sentence-ending punctuation
                for sentence in sentences:
                    cleaned_tokens = clean_and_tokenize(sentence)
                    if cleaned_tokens:  # Avoid adding empty sentences
                        sentences_clean.append(cleaned_tokens)

    return sentences_clean

# Step 3: Calculate word frequencies
def calculate_word_frequencies(sentences):
    """
    Calculates the frequency of each word in the given list of cleaned sentences.
    """
    word_freq = Counter()

    for sentence in sentences:
        word_freq.update(set(sentence))  # Count each brand only once per sentence

    return word_freq

# Step 4: Calculate co-occurrences within a window size
def calculate_co_occurrences(sentences, top_brands, window_size):
    """
    Calculates the co-occurrence of brand pairs within a window size in sentences.
    Returns individual and pair counts for lift calculation.
    Ensures that each brand mention is only counted once per post.
    """
    brand_pair_counts = defaultdict(int)
    brand_counts = Counter()

    for sentence in sentences:
        # print('sentence',sentence)
        # Only keep unique brands in the sentence
        unique_brands = list(set([brand for brand in sentence if brand in top_brands]))
        # print('unique_brands', unique_brands)
        brand_counts.update(set(unique_brands))  # Count each brand only once per sentence

        # Calculate co-occurrence between brands within the window size
        for i, brand1 in enumerate(unique_brands):
            for j in range(i + 1, len(unique_brands)):
                brand2 = unique_brands[j]
                if abs(sentence.index(brand1) - sentence.index(brand2)) <= window_size:
                    # Only count the pair once per sentence
                    brand_pair_counts[(brand1, brand2)] += 1
                    brand_pair_counts[(brand2, brand1)] += 1  # Symmetric count for both pairs

    return brand_counts, brand_pair_counts

# Step 5: Calculate lift ratios
def calculate_lift(brand_counts, brand_pair_counts, total_sentences):
    """
    Calculates the lift ratio for each brand pair based on their co-occurrences.
    Ensures that each brand pair is counted once per post.
    """
    lift_ratios = defaultdict(lambda: None)  # Initialize with None for easier matrix plotting

    for (brand1, brand2), joint_count in brand_pair_counts.items():
        # Calculate individual probabilities for brand1 and brand2
        # print(brand1,brand2)
        prob_A = brand_counts[brand1] / total_sentences
        # print('prob_A', prob_A)
        prob_B = brand_counts[brand2] / total_sentences
        # print('prob_B', prob_B)
        # Calculate joint probability of both brands occurring together
        prob_AB = joint_count / total_sentences
        # print('prob_AB', prob_AB)

        # Calculate lift ratio, ensuring we don't divide by zero
        if prob_A > 0 and prob_B > 0:
            lift = prob_AB / (prob_A * prob_B)
            lift_ratios[(brand1, brand2)] = lift
        else:
            lift_ratios[(brand1, brand2)] = np.NaN

    return lift_ratios

# Step 6: Plot lift ratios as a matrix without a heatmap
def plot_lift_matrix_no_heatmap(lift_ratios, top_brands):
    """
    Plots a matrix of lift ratios for the top-10 brands without heatmap coloring.
    Only text values are shown.
    """
    # Create an empty matrix
    lift_matrix = np.full((len(top_brands), len(top_brands)), "", dtype=object)

    # Populate matrix with lift values
    for i, brand1 in enumerate(top_brands):
        for j, brand2 in enumerate(top_brands):
            if brand1 != brand2:  # Leave diagonal elements empty
                lift_value = lift_ratios.get((brand1, brand2), np.NaN)
                if lift_value is not None:
                    lift_matrix[i, j] = f"{lift_value:.2f}"

    # Plot the matrix using a text-based table
    fig, ax = plt.subplots(figsize=(10, 8))
    ax.set_axis_off()

    # Create table
    table = plt.table(cellText=lift_matrix, rowLabels=top_brands, colLabels=top_brands,
                      cellLoc='center', loc='center')

    # Set the table font size and other properties
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1.2, 1.2)  # Adjust scaling for better visibility

    plt.title('Lift Ratios for Top-10 Car Brands', fontsize=20)
    plt.show()
    return lift_matrix

# Main function to run the process
def main():
    # Step A: Process sentences and calculate word frequencies
    sentences_clean = extract_sentences(input_filename)
    word_freq = calculate_word_frequencies(sentences_clean)

    # Get the top 10 brands based on frequency
    top_10_brands = [brand for brand, _ in word_freq.most_common(10)]

    # Step B: Calculate co-occurrences within the window size
    brand_counts, brand_pair_counts = calculate_co_occurrences(sentences_clean, top_10_brands, window_size)

    # Step C: Calculate lift ratios
    total_sentences = len(sentences_clean)
    lift_ratios = calculate_lift(brand_counts, brand_pair_counts, total_sentences)

    # Step D: Plot lift matrix without heatmap
    lift_matrix=plot_lift_matrix_no_heatmap(lift_ratios, top_10_brands)
    #print(lift_matrix)

    dff=pd.DataFrame(lift_matrix,columns=top_10_brands)
    dff.to_csv('liftmatrix.csv', index=False)


if __name__ == "__main__":
    main()

"""# TASK D"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import MDS
from sklearn.cluster import KMeans
from scipy.spatial import ConvexHull
# Replace NaN with 1s on the diagonal for self-lift values
lift_matrix = pd.read_csv('/content/liftmatrix.csv')
lift_matrix.fillna(1, inplace=True)
# Convert lift values to a dissimilarity matrix (inverse of lift)
dissimilarity_matrix = 1 / lift_matrix
# Apply MDS to reduce the dimensionality
mds = MDS(n_components=2, dissimilarity="precomputed", random_state=42)
mds_coordinates = mds.fit_transform(dissimilarity_matrix)
# Apply KMeans clustering
num_clusters = 3  # You can adjust the number of clusters
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
labels = kmeans.fit_predict(mds_coordinates)
# Plot the MDS coordinates and clusters
plt.figure(figsize=(10, 8))
# Plot each point and draw convex hulls around clusters
for cluster in range(num_clusters):
    cluster_points = mds_coordinates[labels == cluster]
    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f"Cluster {cluster + 1}")
    # Draw convex hull around the cluster points
    if len(cluster_points) > 2:  # Convex hull needs at least 3 points
        hull = ConvexHull(cluster_points)
        for simplex in hull.simplices:
            plt.plot(cluster_points[simplex, 0], cluster_points[simplex, 1], 'k-')
# Label the points with brand names
brand_names = lift_matrix.columns
for i, brand in enumerate(brand_names):
    plt.text(mds_coordinates[i, 0], mds_coordinates[i, 1], brand, fontsize=12)
plt.title('MDS Plot of Car Brands with Clusters')
plt.xlabel('MDS Dimension 1')
plt.ylabel('MDS Dimension 2')
plt.legend()
plt.grid(True)
plt.show()

"""# TASK E

We got 3 distinct clusters:

*Mass-Market Brands:*
  * Toyota, Hyundai, Volkswagen, Ford

*Japanese Premium Brands:*
  * Acura, Infiniti, Honda

*European Luxury Brands:*
  * BMW, Audi, Cadillac

Insights after performing tasks C & D:

1. There is a significant distance between BMW and other
luxury brands like Audi and Cadillac, despite being in the same cluster. This suggests that customers perceive BMW as quite distinct in terms of brand identity or value propositions, which may stem from different brand positioning strategies, such as a stronger emphasis on performance or sportiness.

2. Proximity of Honda to Acura and Infiniti: Despite Honda being a mass-market brand and Acura and Infiniti being luxury brands, they are clustered together. This suggests that Honda's higher-end models may overlap in customer perception with Acura's and Infiniti's entry-level luxury models, indicating potential brand interchangeability for certain customers.

3. Volkswagen's position relative to other mass-market brands: While clustered with Toyota, Hyundai, and Ford, Volkswagen is positioned slightly apart. This could indicate that Volkswagen is perceived as a bridge between mass-market and entry-level premium segments, potentially due to its slightly higher pricing or perceived quality compared to other mass-market brands.

4. The close positioning of Audi and Cadillac suggests that despite their different origins (European vs. American), they may be perceived similarly in terms of luxury positioning or target market. This could indicate that Cadillac's efforts to compete with European luxury brands have been somewhat successful in terms of brand perception.

# TASK F
"""

import pandas as pd
import nltk
from nltk.corpus import stopwords
from collections import Counter
import re

# Read preprocessed data
file_path = '/content/cleaned_replaced_records_forum.csv'
data = pd.read_csv(file_path)

nltk.download('stopwords')

# Define stopwords and updated car-related terms
stop_words = set(stopwords.words('english'))
car_attributes = ['price', 'performance', 'engine', 'transmission', 'speed','exterior', 'reliability', 'safety', 'comfort', 'technology', 'handling']

# Use the Top 5 brands that were calculated in a previous task
top_5_brands = ['bmw', 'audi', 'acura', 'honda', 'volkswagen']

def extract_car_attributes(review, car_attributes):
    review = re.sub(r'\W+', ' ', review.lower())
    words = review.split()
    filtered_words = [word for word in words if word in car_attributes and word not in stop_words]
    return filtered_words

data['Review'] = data['Review'].fillna('')

all_attributes = []
for review in data['Review']:
    all_attributes.extend(extract_car_attributes(review, car_attributes))

# Count frequencies of car attributes
attribute_counts = Counter(all_attributes)

# Get the top 5 attributes
most_common_attributes = attribute_counts.most_common(5)
top_5_attributes = [attr for attr, count in most_common_attributes]
print("Top 5 attributes:", top_5_attributes)

def count_brand_attribute_cooccurrences(data, brands, attributes):
    brand_attr_counts = {brand: {attr: 0 for attr in attributes} for brand in brands}
    brand_counts = {brand: 0 for brand in brands}
    attr_counts = {attr: 0 for attr in attributes}
    total_reviews = 0

    for review in data['Review']:
        # Preprocess the review text
        review = re.sub(r'\W+', ' ', review.lower())
        words = set(review.split())

        # Check if any brand is mentioned
        brands_in_review = words.intersection(brands)
        attrs_in_review = words.intersection(attributes)

        if brands_in_review and attrs_in_review:
            total_reviews += 1
            for brand in brands_in_review:
                brand_counts[brand] += 1
                for attr in attrs_in_review:
                    attr_counts[attr] += 1
                    brand_attr_counts[brand][attr] += 1

    return brand_attr_counts, brand_counts, attr_counts, total_reviews

# Count brand-attribute co-occurrences for the top 5 attributes
brand_attr_counts, brand_counts, attr_counts, total_reviews = count_brand_attribute_cooccurrences(
    data, top_5_brands, top_5_attributes
)

def calculate_lift(brand_attr_counts, brand_counts, attr_counts, total_reviews):
    lifts = {brand: {} for brand in brand_attr_counts}
    for brand in brand_attr_counts:
        for attr in brand_attr_counts[brand]:
            p_brand = brand_counts[brand] / total_reviews
            p_attr = attr_counts[attr] / total_reviews
            p_brand_and_attr = brand_attr_counts[brand][attr] / total_reviews

            if p_brand > 0 and p_attr > 0:
                lift = p_brand_and_attr / (p_brand * p_attr)
                lifts[brand][attr] = lift

    return lifts

# Calculate lift values for the top 5 attributes
lift_values = calculate_lift(brand_attr_counts, brand_counts, attr_counts, total_reviews)

# Print lift values for the top 5 brands and attributes
for brand in top_5_brands:
    print(f"{brand.upper()}:")
    # Get the lift values for the current brand and sort them in descending order
    sorted_attrs = sorted(top_5_attributes, key=lambda attr: lift_values[brand].get(attr, 0), reverse=True)

    for attr in sorted_attrs:
        lift_value = lift_values[brand].get(attr, 'N/A')
        print(f"    {attr.capitalize()} lift: {lift_value}")

"""# TASK G

1. **Brand Positioning Based on Performance and Speed**: Audi and Volkswagen stand out for their association with speed, with Volkswagen leading. In terms of performance, both BMW and Audi rank highly. This suggests that Audi can be positioned as a well-rounded performer, balancing speed with an overall strong driving experience. Volkswagen, however, could focus its marketing on speed, appealing to younger, performance-driven customers who prioritize this attribute.

2. **Transmission and Engine Focus**: Honda and Acura show strong associations with transmission and engine attributes. This implies that customers value these brands for their reliability and engineering. Acura’s strength in engine-related discussions suggests it could emphasize precision engineering in its marketing. Honda’s transmission mentions indicate an opportunity to promote ease of driving and reliability.

3. **Price Sensitivity**: Despite being seen as premium brands, price is a significant factor in discussions about BMW, Audi, and Acura. The relatively high price lift for all brands suggests that even in the entry-level luxury market, consumers remain price-sensitive. JD Power can advise clients to maintain competitive pricing strategies, ensuring that perceived value (in terms of features like performance, engine, and speed) aligns with the price.

4. **Targeting Specific Customer Segments**:
   - **BMW**: Should continue emphasizing performance and luxury, balancing it with pricing to align with consumer value expectations.
   - **Audi**: Can target tech-savvy or younger buyers who want a combination of speed and performance for a dynamic driving experience.
   - **Acura and Honda**: Should focus on reliability and engineering, appealing to practical buyers who value durability in transmission and engine.
   - **Volkswagen**: Can market itself as a performance vehicle with a reasonable price, focusing on speed and affordability.

5. **Differentiation Strategy**: The lift values highlight each brand’s distinct strengths. JD Power should recommend that clients avoid generic marketing approaches and instead focus on the attributes where they excel. For instance, Audi and Volkswagen should lean into their speed-focused branding, while BMW should continue emphasizing its performance advantage.

# TASK H
"""

input_filename = '/content/cleaned_replaced_records_forum.csv'  # Input file path
df = pd.read_csv(input_filename)
brands = pd.read_csv('/content/word_freq.csv')

brands = brands.set_index('Brand')

def preprocess_review(review):

    # Adding aspirational trigrams
    review = re.sub(r"would be thrilled", "aspiration", review)
    review = re.sub(r"have always dreamed", "aspiration", review)
    review = re.sub(r"have always wanted", "aspiration", review)
    review = re.sub(r"am enthusiastic to", "aspiration", review)
    review = re.sub(r"am eager to", "aspiration", review)
    review = re.sub(r"cant wait to", "aspiration", review)
    review = re.sub(r"really want to", "aspiration", review)
    review = re.sub(r"would love to", "aspiration", review)
    review = re.sub(r"would be enchanted", "aspiration", review)
    review = re.sub(r"would love if", "aspiration", review)
    review = re.sub(r"am excited to", "aspiration", review)
    review = re.sub(r"would die to", "aspiration", review)


    # Adding aspirational bigrams
    review = re.sub(r"determined to", "aspiration", review)
    review = re.sub(r"passionate about", "aspiration", review)
    review = re.sub(r"waiting to", "aspiration", review)
    review = re.sub(r"aspire of", "aspiration", review)
    review = re.sub(r"desire to", "aspiration", review)
    review = re.sub(r"keen to", "aspiration", review)
    review = re.sub(r"want to", "aspiration", review)
    review = re.sub(r"dream of", "aspiration", review)
    review = re.sub(r"ambitious to", "aspiration", review)
    review = re.sub(r"love to", "aspiration", review)
    review = re.sub(r"yearn to", "aspiration", review)
    review = re.sub(r"eager to", "aspiration", review)
    review = re.sub(r"wish to", "aspiration", review)
    review = re.sub(r"crave to", "aspiration", review)
    review = re.sub(r"intend to", "aspiration", review)
    review = re.sub(r"dying to", "aspiration", review)
    review = re.sub(r"intend on", "aspiration", review)
    review = re.sub(r"looking to", "aspiration", review)
    review = re.sub(r"long to", "aspiration", review)
    review = re.sub(r"excited to", "aspiration", review)
    review = re.sub(r"ready to", "aspiration", review)
    review = re.sub(r"set on", "aspiration", review)
    review = re.sub(r"aspiring to", "aspiration", review)
    review = re.sub(r"itching to", "aspiration", review)
    review = re.sub(r"aim to", "aspiration", review)
    review = re.sub(r"willing to", "aspiration", review)
    review = re.sub(r"hope to", "aspiration", review)
    review = re.sub(r"my dream", "aspiration", review)
    review = re.sub(r"plan to", "aspiration", review)
    review = re.sub(r"aim to", "aspiration", review)
    return review

df['Review'] = df['Review'].astype(str).apply(preprocess_review)

def liftCalc_with_aspire(word_1, word_2):
    w1freq = sum(df.apply(lambda x: word_1 in x['Review'], axis=1))
    w2freq = sum(df.apply(lambda x: word_2 in x['Review'], axis=1))
    bothfreq = sum(df.apply(lambda x: word_1 in x['Review'] and word_2 in x['Review'], axis=1))

    if w1freq == 0 or w2freq == 0:
        return 0

    return float(bothfreq/len(df))/((float(w1freq)/len(df))*(float(w2freq)/len(df)))

brand_aspire_lifts = {}

for brand in brands.index:
    lift = liftCalc_with_aspire(brand, 'aspiration')
    brand_aspire_lifts[brand] = lift

df_lifts = pd.DataFrame(list(brand_aspire_lifts.items()), columns=['Brand', 'Lift'])
df_lifts['Lift'] = df_lifts['Lift'].round(2)

print(df_lifts)

df_lifts.sort_values(by = 'Lift').plot(x = 'Brand', y='Lift', kind = 'bar')

"""**Cadillac** and **Infiniti** show the highest lift values, indicating that they are the most aspirational brands in the dataset, with users frequently associating them with positive aspirations or desires. **Acura** and **Honda** also rank highly, reflecting significant consumer interest and aspirational appeal."""